{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcWuQO3P43d0ThAcpokHC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luoshaoyang/cs224w_final_proj/blob/main/Colab_FinalProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Visible Masking/Watermark Removal with Graph Neural Network"
      ],
      "metadata": {
        "id": "Xbd5iJ6Qz0nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8S5SXBCzyUz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "FOLDERNAME = 'CS224WFinalProj/'\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
        "%cd /content/drive/MyDrive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "* Import Libraries\n",
        "* Handy Functions<br>\n",
        " 1. loadImage\n",
        " 2. image2NN\n",
        " 3. NN2Image\n",
        " 4. plotImage\n",
        " 5. image2GrayScale\n",
        " 6. image2Binary\n",
        " 7. overlayerImg\n",
        " 8. overlayImg1OverImg2\n",
        " 9. addWatermark"
      ],
      "metadata": {
        "id": "i4YtLqLjz7nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-geometric\n",
        "from torch_geometric.data import Data\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device"
      ],
      "metadata": {
        "id": "V-5SYxN7z8j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "def loadImage(path,plotImg=True,gray_scale=False):\n",
        "  #####################################################\n",
        "  #Read in image path and returns an image\n",
        "  #####################################################\n",
        "  image = cv2.imread(path)\n",
        "  image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY if gray_scale else cv2.COLOR_BGR2RGB)  \n",
        "  if plotImg:\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "  return image\n",
        "\n",
        "def image2Tensor(img,gray_scale=True):\n",
        "  #####################################################\n",
        "  #Read in image, and return tensor of the image\n",
        "  #  more interesting reading: https://towardsdatascience.com/image-read-and-resize-with-opencv-tensorflow-and-pil-3e0f29b992be\n",
        "  #####################################################\n",
        "  # convert BGR image to RGB image\n",
        "  img = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY if gray_scale else cv2.COLOR_BGR2RGB)\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  img_tensor = transform(img)\n",
        "  return img_tensor\n",
        "\n",
        "def Tensor2Image(img_tensor,dim_order=None,gray_scale=True,plotImg=True):\n",
        "  #####################################################\n",
        "  #Read in tensor, and return plotable image\n",
        "  #####################################################\n",
        "  img = img_tensor.numpy()\n",
        "  if dim_order:\n",
        "    img = np.transpose(img,order)\n",
        "  img = cvtColor(img,cv2.COLOR_BGR2GRAY if gray_scale else cv2.COLOR_BGR2RGB)\n",
        "  if plotImg:\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "  return img\n",
        "\n",
        "def plotImg(image):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def image2GrayScale(image):\n",
        "  img = cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  return img\n",
        "\n",
        "def image2Binary(image):\n",
        "  img = image2GrayScale(image)\n",
        "  img = np.round(img)\n",
        "  return img\n",
        "\n",
        "def overlayerImg(size,rgb):\n",
        "  #####################################################\n",
        "  #Create a box overlay image that could go above the license plate\n",
        "  #  rgb being a vector of shape (,3); size being a vector of shape (row,col,3)\n",
        "  #####################################################\n",
        "  img = np.ndarray([[[rgb[0]]*size.shape[0]]*size.shape[1],\n",
        "                    [[rgb[1]]*size.shape[0]]*size.shape[1],\n",
        "                    [[rgb[2]]*size.shape[0]]*size.shape[1]])\n",
        "  return img\n",
        "\n",
        "def overlayImg1OverImg2(img1,img2):\n",
        "  #####################################################\n",
        "  #Overlay the box image (img1) above the license plate(img2)\n",
        "  #  center the images at the same place\n",
        "  #####################################################\n",
        "  img = img2.copy()\n",
        "  y_lower = np.round(img.shape[0]/2)-np.round(img1.shape[0]/2)\n",
        "  y_upper = y_lower+img1.shape[0]# (y_lower,y_upper]\n",
        "  img[y_lower:y_upper,:,:] = img1\n",
        "  return img\n",
        "\n",
        "def addWatermark(img,watermark,wm_scale):\n",
        "  #####################################################\n",
        "  #Add watermark image on bottom right of the license plate(img)\n",
        "  #  center the images at the same place\n",
        "  #####################################################\n",
        "  img_wm = cv2.resize(img,(148,148),interpolation=cv2.INTER_AREA)\n",
        "  img_resize=img_wm.copy()\n",
        "  #prepare watermark image\n",
        "  wm_size = (100,50)\n",
        "  img_w = cv2.resize(watermark,wm_size,interpolation=cv2.INTER_AREA)\n",
        "  #prepare cordinates\n",
        "  h_img, w_img, _ = img_wm.shape\n",
        "  center_y = int(h_img/2)\n",
        "  center_x = int(w_img/2)\n",
        "  h_wm, w_wm, _ = img_w.shape\n",
        "  top_y = center_y - int(h_wm/2)\n",
        "  left_x = center_x - int(w_wm/2)\n",
        "  bottom_y = top_y + h_wm\n",
        "  right_x = left_x + w_wm\n",
        "  roi = img_wm[top_y:bottom_y, left_x:right_x]\n",
        "  result = cv2.addWeighted(roi, 1, img_w, 1, 0)\n",
        "  img_wm[top_y:bottom_y, left_x:right_x] = result\n",
        "  return (img_resize,img_wm)"
      ],
      "metadata": {
        "id": "DUEJT_NM0BiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "watermark = loadImage(\"watermark.png\")\n",
        "img = loadImage(\"img.jpg\")\n",
        "wm = addWatermark(img,watermark,wm_scale=1)\n",
        "#plt.imshow(cv2.resize(img,(148,148),interpolation=cv2.INTER_AREA))\n",
        "plt.imshow(wm[1])"
      ],
      "metadata": {
        "id": "EdukCWA00D7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing\n",
        "* Read in the datasets\n",
        "* Create masked/watermarked datasets\n",
        "* Split for training and testing"
      ],
      "metadata": {
        "id": "mvvt3_RZ0HOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataSet\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test\n",
        "# idx=0#for image 0 in training\n",
        "# path = os.path.join('../data', train.loc[idx, 'Label'], train.loc[idx, 'Image'])\n",
        "# loadImage(path)"
      ],
      "metadata": {
        "id": "qOkXhSgR0HxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create watermarked datasets: bottom right 100*100 pixels\n",
        "watermark = loadImage(\"watermark.png\")\n",
        "\n",
        "train_wm={}#{file_name:img}\n",
        "train_original={}#{file_name:img}\n",
        "train_names=[]\n",
        "counter=0\n",
        "img_file=\"4f858f20ecfe17.jpg\"\n",
        "for img_file in train[\"Image\"]:\n",
        "  path = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data',img_file)\n",
        "  path_wm = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data/train/wm',img_file)\n",
        "  path_original = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data/train/original',img_file)\n",
        "  try:\n",
        "    img = loadImage(path,plotImg=False)\n",
        "    img_wm = addWatermark(img,watermark,wm_scale=0.04)\n",
        "    # train_original[img_file]=img.copy()\n",
        "    # train_wm[img_file]=img_wm.copy()\n",
        "    #cv2.imwrite(path_original,img)\n",
        "    cv2.imwrite(path_wm,img_wm[1])\n",
        "    train_names.append(img_file)\n",
        "    # del img,img_wm\n",
        "  except:\n",
        "    print(f\"{img_file} not exist, skip.\")\n",
        "    continue\n",
        "  counter+=1\n",
        "  #recycle RAM\n",
        "  if (counter*100/len(train))%5==0:\n",
        "    print(f\"{(counter*100/len(train))}% training data processed\")\n",
        "    # with open(f\"train_wm_{counter*20/len(train)}.pkl\",\"wb\") as fl:\n",
        "    #   pickle.dump(train_wm,fl)\n",
        "    # print(f\"train_wm_{counter*20/len(train)} saved\")\n",
        "    # with open(f\"train_original_{counter*20/len(train)}.pkl\",\"wb\") as fl:\n",
        "    #   pickle.dump(train_original,fl)\n",
        "    # print(f\"train_original_{counter*20/len(train)} saved\")\n",
        "    # del train_wm,train_original\n",
        "    # train_wm={}\n",
        "    # train_original={}\n",
        "\n",
        "# with open(\"train_wm.pkl\",\"wb\") as fl:\n",
        "#   pickle.dump(train_wm,fl)\n",
        "#   print(\"train_wm saved\")\n",
        "# with open(\"train_original.pkl\",\"wb\") as fl:\n",
        "#   pickle.dump(train_original,fl)\n",
        "#   print(\"train_original saved\")\n",
        "\n",
        "counter=0\n",
        "test_wm={}#{file_name:img}\n",
        "test_original={}#{file_name:img}\n",
        "test_names=[]\n",
        "for img_file in test[\"Image\"]:\n",
        "  path = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data',img_file)\n",
        "  path_wm = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data/test/wm',img_file)\n",
        "  path_original = os.path.join('/content/drive/MyDrive/CS224WFinalProj/data/test/original',img_file)\n",
        "  try:\n",
        "    img = loadImage(path,plotImg=False)\n",
        "    img_wm = addWatermark(img,watermark,wm_scale=1)\n",
        "    # test_original[img_file]=img.copy()\n",
        "    # test_wm[img_file]=img_wm.copy()\n",
        "    #cv2.imwrite(path_original,img)\n",
        "    cv2.imwrite(path_wm,img_wm[1])\n",
        "    test_names.append(img_file)\n",
        "  except:\n",
        "    print(f\"{img_file} not exist, skip.\")\n",
        "    continue\n",
        "  if (counter*100/len(test))%5==0:\n",
        "    print(f\"{(counter*100/len(test))}% testing data processed\")\n",
        "  counter+=1\n",
        "# with open(\"test_wm.pkl\",\"wb\") as fl:\n",
        "#   pickle.dump(test_wm,fl)\n",
        "#   print(\"test_wm saved\")\n",
        "# with open(\"test_original.pkl\",\"wb\") as fl:\n",
        "#   pickle.dump(test_original,fl)\n",
        "#   print(\"test_original saved\")\n",
        "\n",
        "# # load pre-processed data\n",
        "# with open(\"train_wm.pkl\",\"wb\") as fl:\n",
        "#   train_wm=pickle.load(fl)\n",
        "# with open(\"train_original.pkl\",\"wb\") as fl:\n",
        "#   train_original=pickle.load(fl)\n",
        "# with open(\"test_wm.pkl\",\"wb\") as fl:\n",
        "#   test_wm=pickle.load(fl)\n",
        "# with open(\"test_original.pkl\",\"wb\") as fl:\n",
        "#   test_original=pickle.load(fl)"
      ],
      "metadata": {
        "id": "zslCgogJ0P__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Build\n",
        "* Learn GNN embedding\n",
        "* Apply GCN to predict<br>\n",
        "  1. GCN training\n",
        "  2. Accuracy measurement\n",
        "* Apply label propagation to predict<br>\n",
        "  1. Edge weight training\n",
        "  2. propagation iteration\n",
        "  3. Accuracy measurement"
      ],
      "metadata": {
        "id": "K4hAdhAv0S3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Propagation\n",
        "#####################################\n",
        "#prepare edge_weight and nn graph\n",
        "#- dict edge_weights = {(from_node_id,to_node_id):weight}\n",
        "edge_weights = {\n",
        "    (1,2):1,(2,1):1,\n",
        "    (2,3):1,(3,2):1,\n",
        "    (3,4):1,(4,3):1,\n",
        "    (4,0):1,(0,4):1,\n",
        "    (0,1):1,(1,0):1\n",
        "}\n",
        "#- list edges = [(from_node_id,to_node_id)]\n",
        "edges = edge_weights.keys()\n",
        "#- dict node_featuress = {node_id:[r,g,b]}\n",
        "node_features = {\n",
        "    1:np.array([1.,0.,0.]),\n",
        "    2:np.array([0.,1.,0.]),\n",
        "    3:np.array([0.,0.,1.]),\n",
        "    4:np.array([1.,1.,0.]),\n",
        "    0:np.array([1.,1.,1.])\n",
        "}\n",
        "#- dict node_degree_in = {node_id:int}\n",
        "#- dict node_degree_out = {node_id:int}\n",
        "node_degree_in = {}\n",
        "node_degree_out = {}\n",
        "for relation in edge_weights.keys():\n",
        "  if relation[0] in node_degree_out.keys():\n",
        "    node_degree_out[relation[0]] += 1\n",
        "  else:\n",
        "    node_degree_out[relation[0]] = 1\n",
        "  if relation[1] in node_degree_in.keys():\n",
        "    node_degree_in[relation[1]] += 1\n",
        "  else:\n",
        "    node_degree_in[relation[1]] = 1\n",
        "#- dict mask_label = {node_id:1/0}\n",
        "mask_label = {\n",
        "    1:0,\n",
        "    2:1,\n",
        "    3:0,\n",
        "    4:1,\n",
        "    0:0\n",
        "}\n",
        "#- array adjacency as a to_node*from_node matix\n",
        "adjacency = np.zeros((5,5))\n",
        "for relation in edges:\n",
        "  adjacency[relation[1],relation[0]]=1\n",
        "#####################################\n",
        "#vanilla label propagation\n",
        "epsilon = 0.01\n",
        "max_iter = 10000\n",
        "#function to calculate node feature differences\n",
        "def featureDiff(iter_features,curr_features,agg=max):\n",
        "  assert iter_features.keys() == curr_features.keys()\n",
        "  diff_dict = {}\n",
        "  for node in iter_features.keys():\n",
        "    diff_dict[node] = agg(curr_features[node]-iter_features[node])\n",
        "  return diff_dict\n",
        "\n",
        "iter_features = node_features.copy()\n",
        "for i in range(max_iter):\n",
        "  curr_features = iter_features.copy()\n",
        "  #update label for masked nodes only\n",
        "  for relation in edge_weights.keys():\n",
        "    if mask_label[relation[1]]:\n",
        "      curr_features[relation[1]] += iter_features[relation[0]]*edge_weights[relation]/node_degree_in[relation[1]]\n",
        "  #measure difference\n",
        "  errors = featureDiff(iter_features,curr_features)\n",
        "  iter_features = curr_features.copy()\n",
        "  if max(errors.values())<epsilon:\n",
        "    break\n",
        "\n",
        "#####################################\n",
        "#correct & smooth\n",
        "epsilon = 0.01\n",
        "max_iter = 3\n",
        "alpha = 0.1\n",
        "diffusion_scale = 0.1\n",
        "#function to calculate diffusion matrix\n",
        "def adjDiffuse(adjacency):\n",
        "  diffusion = adjacency.copy()\n",
        "  for i in range(diffusion.shape[0]):\n",
        "    for j in range(diffusion.shape[1]):\n",
        "      w_ij = diffusion[i,:].sum()*diffusion[:,j].sum()\n",
        "      w_ij = 1 if w_ij==0 else w_ij\n",
        "      diffusion[i,j] = diffusion[i,j]/(w_ij)**0.5\n",
        "  return diffusion\n",
        "#function to calculate correction\n",
        "def correctStep(adjacency,errors,alpha=alpha,max_iter=max_iter,epsilon=epsilon,agg=max):\n",
        "  #dimension assersion\n",
        "  assert errors.shape[0]==adjacency.shape[0]\n",
        "  diffusion = adjDiffuse(adjacency)\n",
        "  e = errors.copy()\n",
        "  for i in range(max_iter):\n",
        "    curr_e = e.copy()\n",
        "    curr_e = (1-alpha)*e+alpha*diffusion.dot(e)\n",
        "    if (curr_e-e).max()<=epsilon:\n",
        "      break\n",
        "  return e\n",
        "#function to do smooth step\n",
        "def smoothStep(adjacency,preds,alpha=alpha,max_iter=max_iter,epsilon=epsilon,agg=max):\n",
        "  #dimension assersion\n",
        "  assert preds.shape[0]==adjacency.shape[0]\n",
        "  diffusion = adjDiffuse(adjacency)\n",
        "  z = preds.copy()\n",
        "  for i in range(max_iter):\n",
        "    curr_z = z.copy()\n",
        "    curr_z = (1-alpha)*z+alpha*diffusion.dot(z)\n",
        "    if (curr_z-z).max()<=epsilon:\n",
        "      break\n",
        "  return z\n",
        "\n",
        "#[ToGet]training predictions\n",
        "node_feature_array = np.array([node_features[node] for node in sorted(node_features.keys())])\n",
        "mask_label_array = np.array([i for i in mask_label.values()])\n",
        "pred_features = np.random.uniform(size=node_feature_array.shape)\n",
        "#correct step\n",
        "training_erorrs = (node_feature_array-pred_features)*(1-mask_label_array).reshape((len(mask_label_array),1))\n",
        "correction = correctStep(adjacency,errors=training_erorrs)\n",
        "pred_features += diffusion_scale*correction\n",
        "#smooth step\n",
        "pred_features *= mask_label_array.reshape((len(mask_label_array),1))\n",
        "pred_features += node_feature_array*(1-mask_label_array).reshape((len(mask_label_array),1))\n",
        "pred_features = smoothStep(adjacency,preds=pred_features)\n"
      ],
      "metadata": {
        "id": "nTewMusQ0TUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functin to do vanilla label propagation on a predicted image\n",
        "def labelPropagation(node_features,mask_label,edges,max_iter=1000,epsilon=0.01,agg=max):\n",
        "  #############################################\n",
        "  #INPUT\n",
        "  #- node_features: dict of predictd values {node_id:feature_vector}\n",
        "  #- mask_label:  dict of whether the nodes are potentially water-marked {node_id:0/1}\n",
        "  #- edges: list of edges [(from_node_id,to_node_id)]\n",
        "  #- max_iter: int of max number of propagation iterations before convergence\n",
        "  #- epsilon: float tolerance of changes on feature vectors per iteration\n",
        "  #- agg: function used to calculate the feature difference\n",
        "  #OUTPUT\n",
        "  #- lp_features: dict of label-propagated values {node_id:feature_vector}\n",
        "  #############################################\n",
        "  def featureDiff(iter_features,curr_features,agg=max):\n",
        "    assert iter_features.keys() == curr_features.keys()\n",
        "    diff_dict = {}\n",
        "    for node in iter_features.keys():\n",
        "      diff_dict[node] = agg(curr_features[node]-iter_features[node])\n",
        "    return diff_dict\n",
        "  \n",
        "  # initialize useful graph metrics\n",
        "  iter_features = node_features.copy()\n",
        "  edge_weights = {edge:1 for edge in edges}\n",
        "  node_degree_in = {}\n",
        "  node_degree_out = {}\n",
        "  for relation in edge_weights.keys():\n",
        "    if relation[0] in node_degree_out.keys():\n",
        "      node_degree_out[relation[0]] += 1\n",
        "    else:\n",
        "      node_degree_out[relation[0]] = 1\n",
        "    if relation[1] in node_degree_in.keys():\n",
        "      node_degree_in[relation[1]] += 1\n",
        "    else:\n",
        "      node_degree_in[relation[1]] = 1\n",
        "  \n",
        "  for i in range(max_iter):\n",
        "    curr_features = iter_features.copy()\n",
        "    #update label for masked nodes only\n",
        "    for relation in edges:\n",
        "      if mask_label[relation[1]]:\n",
        "        curr_features[relation[1]] = curr_features[relation[1]] + iter_features[relation[0]]*edge_weights[relation]/node_degree_in[relation[1]]\n",
        "    #measure difference\n",
        "    errors = featureDiff(iter_features,curr_features)\n",
        "    iter_features = curr_features.copy()\n",
        "    if max(errors.values())<epsilon:\n",
        "      break\n",
        "  \n",
        "  #convert into dictionary\n",
        "  lp_features = {}\n",
        "  for i in range(len(iter_features)):\n",
        "    lp_features[i] = iter_features[i]\n",
        "\n",
        "  return iter_features\n",
        "\n",
        "# function to do C&S on the predictions\n",
        "def correctSmooth(node_features,pred_features,mask_label,edges,alpha=0.1,max_iter=3,epsilon=0.01,agg=max,diffusion_scale=0.1):\n",
        "  #############################################\n",
        "  #INPUT\n",
        "  #- node_features: dict of before training values {node_id:feature_vector}\n",
        "  #- pred_features: dict of predictd values {node_id:feature_vector}\n",
        "  #- mask_label:  dict of whether the nodes are potentially water-marked {node_id:0/1}\n",
        "  #- edges: list of edges [(from_node_id,to_node_id)]\n",
        "  #- alpha: float of weight of diffusion\n",
        "  #- max_iter: int of max number of propagation iterations before convergence\n",
        "  #- epsilon: float tolerance of changes on feature vectors per iteration\n",
        "  #- agg: function used to calculate the feature difference\n",
        "  #- diffusion_scale: float of scale on correct step\n",
        "  #OUTPUT\n",
        "  #- lp_features: dict of label-propagated values {node_id:feature_vector}\n",
        "  #############################################\n",
        "  #function to calculate the normalized diffusion matrix\n",
        "  def adjDiffuse(adjacency):\n",
        "    diffusion = adjacency.copy()\n",
        "    isum = adjacency.sum(axis=1)**0.5\n",
        "    jsum = adjacency.sum(axis=0)**0.5\n",
        "    for i in range(diffusion.shape[0]):\n",
        "      diffusion[i,:] = diffusion[i,:]/(1 if isum[i] ==0 else isum[i])\n",
        "    for j in range(diffusion.shape[1]):\n",
        "      diffusion[:,j] = diffusion[:,j]/(1 if jsum[j] ==0 else jsum[j])\n",
        "    return diffusion\n",
        "\n",
        "  #function to calculate correction\n",
        "  def correctStep(adjacency,errors,alpha=alpha,max_iter=max_iter,epsilon=epsilon,agg=max):\n",
        "    #dimension assersion\n",
        "    assert errors.shape[0]==adjacency.shape[0]\n",
        "    diffusion = adjDiffuse(adjacency)\n",
        "    e = errors.copy()\n",
        "    for i in range(max_iter):\n",
        "      curr_e = e.copy()\n",
        "      curr_e = (1-alpha)*e+alpha*diffusion.dot(e)\n",
        "      if (curr_e-e).max()<=epsilon:\n",
        "        break\n",
        "    return e\n",
        "\n",
        "  #function to calculate smoothed predictions\n",
        "  def smoothStep(adjacency,preds,alpha=alpha,max_iter=max_iter,epsilon=epsilon,agg=max):\n",
        "    #dimension assersion\n",
        "    assert preds.shape[0]==adjacency.shape[0]\n",
        "    diffusion = adjDiffuse(adjacency)\n",
        "    z = preds.copy()\n",
        "    for i in range(max_iter):\n",
        "      curr_z = z.copy()\n",
        "      curr_z = (1-alpha)*z+alpha*diffusion.dot(z)\n",
        "      if (curr_z-z).max()<=epsilon:\n",
        "        break\n",
        "    return z\n",
        "\n",
        "  node_feature_array = np.array([node_features[node] for node in sorted(node_features.keys())])\n",
        "  pred_features_array = np.array([pred_features[node] for node in sorted(pred_features.keys())])\n",
        "  mask_label_array = np.array([i for i in mask_label.values()])\n",
        "  #array adjacency as a to_node*from_node matix\n",
        "  adjacency = np.zeros((len(mask_label),len(mask_label)))\n",
        "  for relation in edges:\n",
        "    adjacency[relation[1],relation[0]]=1\n",
        "  \n",
        "  #correct step\n",
        "  training_erorrs = (node_feature_array-pred_features_array)*(1-mask_label_array).reshape((len(mask_label_array),1))\n",
        "  correction = correctStep(adjacency,errors=training_erorrs)\n",
        "  pred_features_array += diffusion_scale*correction\n",
        "  #smooth step\n",
        "  pred_features_array *= mask_label_array.reshape((len(mask_label_array),1))\n",
        "  pred_features_array += node_feature_array*(1-mask_label_array).reshape((len(mask_label_array),1))\n",
        "  pred_features_array = smoothStep(adjacency,preds=pred_features_array)\n",
        "  #convert into dictionary\n",
        "  lp_features = {}\n",
        "  for i in range(len(pred_features_array)):\n",
        "    lp_features[i] = pred_features_array[i]\n",
        "  \n",
        "  return pred_features"
      ],
      "metadata": {
        "id": "jcpM871n0V-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# Example of Label Propagation (C&S)\n",
        "#################################################\n",
        "watermark = loadImage(\"watermark.png\")\n",
        "img = loadImage(\"img.jpg\")\n",
        "wm = addWatermark(img,watermark,wm_scale=1)\n",
        "plt.imshow(wm[1])\n",
        "\n",
        "# Copy of image_to_graph function\n",
        "def image_to_graph(image):\n",
        "  h, w = image.shape[:2]\n",
        "  num_nodes = h * w\n",
        "  edge_index = []\n",
        "  for i in range(h):\n",
        "    for j in range(w):\n",
        "      node_id = i * w + j\n",
        "      # Connect to the left pixel\n",
        "      if j > 0:\n",
        "        edge_index.append([node_id, node_id - 1])\n",
        "      # Connect to the right pixel\n",
        "      if j < w - 1:\n",
        "        edge_index.append([node_id, node_id + 1])\n",
        "      # Connect to the up pixel\n",
        "      if i > 0:\n",
        "        edge_index.append([node_id, node_id - w])\n",
        "      # Connect to the bottom pixel\n",
        "      if i < h - 1:\n",
        "        edge_index.append([node_id, node_id + w])\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "  x = torch.tensor(image.reshape(-1, image.shape[-1]), dtype=torch.float)\n",
        "  return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "img_resized = cv2.resize(img,(148,148))\n",
        "imgG = image_to_graph(img_resized)\n",
        "wmG = image_to_graph(wm[1])\n",
        "h,w,_ = img_resized.shape\n",
        "wm_lp = labelPropagation(node_features={i*w+j:wm[1][i,j,:] for i in range(h) for j in range(w)},\n",
        "                      mask_label={i*w+j:1 if 24<=j<=124 and 49<=i<=99 else 0 for i in range(h) for j in range(w)},\n",
        "                      edges=[(int(edge[0]),int(edge[1])) for edge in imgG.edge_index.t()],\n",
        "                      max_iter=10,\n",
        "                      epsilon=0.01\n",
        "                      )\n",
        "plt.figure()\n",
        "plt.imshow(np.array([v for v in wm_lp.values()]).reshape(148,148,3))\n",
        "# wm_cs = correctSmooth(node_features={i*w+j:img_resized[i,j,:] for i in range(h) for j in range(w)},\n",
        "#                       pred_features={i*w+j:wm[1][i,j,:] for i in range(h) for j in range(w)},\n",
        "#                       mask_label={i*w+j:1 if 24<=j<=124 and 49<=i<=99 else 0 for i in range(h) for j in range(w)},\n",
        "#                       edges=[(int(edge[0]),int(edge[1])) for edge in imgG.edge_index.t()],\n",
        "#                       alpha=0.1,#tunable hyper paremeter\n",
        "#                       max_iter=3,#tunable hyper paremeter\n",
        "#                       epsilon=0.01,\n",
        "#                       diffusion_scale=0.1#tunable hyper paremeter\n",
        "#                       )\n",
        "# plt.figure()\n",
        "# plt.imshow(np.array([v for v in wm_cs.values()]).reshape(148,148,3))"
      ],
      "metadata": {
        "id": "Qyc1FRe80Zcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}